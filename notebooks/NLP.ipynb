{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "78bea88238b359cb8720bd7df922336c12f3831db9fec1bf346c95153cd214da"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-f7e445b9a367>, line 38)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f7e445b9a367>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    target_dataset.append(0\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# revisar codigo no estudiado\n",
    "\n",
    "import sys\n",
    "\n",
    "# open files\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:   \n",
    "        target_dataset.append(0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identity matrix\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\na * identity matrix\n[1. 2. 3.]\n\nb * identity matrix\n[0.1 0.2 0.3]\n\nc * identity matrix\n[-1.   0.5  0. ]\n"
     ]
    }
   ],
   "source": [
    "#indentity matrices\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,0.5,0])\n",
    "d = np.array([0,0,0])\n",
    "\n",
    "identity_matrix = np.eye(3)\n",
    "print(\"identity matrix\")\n",
    "print(identity_matrix)\n",
    "print(\"\")\n",
    "print(\"a * identity matrix\")\n",
    "print(a.dot(identity_matrix))\n",
    "print(\"\")\n",
    "print(\"b * identity matrix\")\n",
    "print(b.dot(identity_matrix))\n",
    "print(\"\")\n",
    "print(\"c * identity matrix\")\n",
    "print(c.dot(identity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# forward propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "#word embeddings (vocabulary data)\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0.,0.,0.]])\n",
    "word_vects['bears'] = np.array([[0.,0.,0.]])\n",
    "word_vects['braves'] = np.array([[0.,0.,0.]])\n",
    "word_vects['red'] = np.array([[0.,0.,0.]])\n",
    "word_vects['sox'] = np.array([[0.,0.,0.]])\n",
    "word_vects['lose'] = np.array([[0.,0.,0.]])\n",
    "word_vects['defeat'] = np.array([[0.,0.,0.]])\n",
    "word_vects['beat'] = np.array([[0.,0.,0.]])\n",
    "word_vects['tie'] = np.array([[0.,0.,0.]])\n",
    "\n",
    "# sentence embedding to output clasification weights\n",
    "sent2output = np.random.rand(3,len(word_vects))\n",
    "\n",
    "#transition weights \n",
    "identity = np.eye(3)\n",
    "\n",
    "#creating a sentence embeddings\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "#predict over the vocabulary\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,0,0,0,0,0,0,0,0])\n",
    "\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta = layer_1_delta * 1\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "alpha = 0.01\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['sox'] -= sox_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "identity -= np.outer(layer_0,layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2,pred_delta) * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-31-661a300017cd>, line 74)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-661a300017cd>\"\u001b[0;36m, line \u001b[0;32m74\u001b[0m\n\u001b[0;31m    new_hidden_delta = layer['output_delta']\\.dot(decoder.transpose\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# training a neural network for babi dataset\n",
    "\n",
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('../datasets/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "#utils functions\n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "       idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
    "recurrent = np.eye(embed_size)\n",
    "start = np.zeros(embed_size)\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) +\\\n",
    "                                                embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss\n",
    "\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers,loss = predict(sent)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "\n",
    "        if(layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta']\\.dot(decoder.transpose \n",
    "            ())\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta']\\.dot(recurrent.transpose())\n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta']\\.dot(recurrent.transpose())\n",
    "        \n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    \n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'],\\layer['output_delta']) * alpha / float(len(sent))\n",
    "\n",
    "        embed_idx = sent[layer_idx]\n",
    "\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * \\alpha /float(len(sent))\n",
    "\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'],\\layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "    if(iter % 1000 == 0):\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}